{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ada880c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4215679c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting youtube-transcript-api\n",
      "  Obtaining dependency information for youtube-transcript-api from https://files.pythonhosted.org/packages/52/42/5f57d37d56bdb09722f226ed81cc1bec63942da745aa27266b16b0e16a5d/youtube_transcript_api-0.6.2-py3-none-any.whl.metadata\n",
      "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from youtube-transcript-api) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests->youtube-transcript-api) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests->youtube-transcript-api) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests->youtube-transcript-api) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ahmed\\anaconda3\\lib\\site-packages (from requests->youtube-transcript-api) (2024.2.2)\n",
      "Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
      "Installing collected packages: youtube-transcript-api\n",
      "Successfully installed youtube-transcript-api-0.6.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b0b18a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ReadTimeoutError(\"HTTPSConnectionPool(host='pypi.org', port=443): Read timed out. (read timeout=15)\")': /simple/pytube/\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "!pip install -q pytube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b6f8822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0a1ee77",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import YoutubeLoader\n",
    "\n",
    "transcripts = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac04cf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ylist = [\"https://www.youtube.com/watch?v=vT1JzLTH4G4&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=1&t=2319s&pp=iAQB\",\n",
    "\"https://www.youtube.com/watch?v=OoUX-nOEjG0&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=2&t=2598s&pp=iAQB\",\n",
    "\"https://www.youtube.com/watch?v=h7iBpEHGVNc&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=3&t=2788s&pp=iAQB\",\n",
    "\"https://www.youtube.com/watch?v=d14TUNcbn1k&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=4&pp=iAQB\",\n",
    "\"https://www.youtube.com/watch?v=bNb2fEVKeEo&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=5&pp=iAQB\",\n",
    "\"https://www.youtube.com/watch?v=wEoyxE0GP2M&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=6&pp=iAQB\",\n",
    "\"https://www.youtube.com/watch?v=_JB0AO7QxSA&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=7&t=26s&pp=iAQB\",\n",
    "\"https://www.youtube.com/watch?v=6SlgtELqOWc&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=8&t=18s&pp=iAQB\",\n",
    "\"https://www.youtube.com/watch?v=DAOcjicFr1Y&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=9&pp=iAQB\",\n",
    "\"https://www.youtube.com/watch?v=6niqTuYFZLQ&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=10&pp=iAQB\",\n",
    "\"https://www.youtube.com/watch?v=nDPWywWRIRo&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=11&pp=iAQB\",\n",
    "\"https://www.youtube.com/watch?v=6wcs6szJWMY&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=12&t=2161s&pp=iAQB\",\n",
    "\"https://www.youtube.com/watch?v=5WoItGTWV54&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=13&pp=iAQB\",\n",
    "\"https://www.youtube.com/watch?v=lvoHnicueoE&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=14&t=2707s&pp=iAQB\",\n",
    "\"https://www.youtube.com/watch?v=eZdOkDtYMoo&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=15&pp=iAQB\",\n",
    "\"https://www.youtube.com/watch?v=CIfsB_EYsVI&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=16&pp=iAQB\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27062081",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ylist)):\n",
    "    loader = YoutubeLoader.from_youtube_url(ylist[i], add_video_info=True).load()\n",
    "    transcripts.append(loader)\n",
    "    time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3ddc449",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(page_content=\"- Okay so welcome to\\nCS 231N Lecture three. Today we're going to talk about\\nloss functions and optimization but as usual, before we\\nget to the main content of the lecture, there's a\\ncouple administrative things to talk about. So the first thing is that\\nassignment one has been released. You can find the link up on the website. And since we were a little bit late in getting this assignment\\nout to you guys, we've decided to change\\nthe due date to Thursday, April 20th at 11:59 p.m., this will give you a full\\ntwo weeks from the assignment release date to go and\\nactually finish and work on it, so we'll update the syllabus\\nfor this new due date in a little bit later today. And as a reminder, when you\\ncomplete the assignment, you should go turn in the\\nfinal zip file on Canvas so we can grade it and get\\nyour grades back as quickly as possible. So the next thing is always\\ncheck out Piazza for interesting administrative stuff. So this week I wanted to\\nhighlight that we have several example project ideas as\\na pinned post on Piazza. So we went out and solicited\\nexample of project ideas from various people in the\\nStanford community or affiliated to Stanford, and they came\\nup with some interesting suggestions for projects\\nthat they might want students in the class to work on. So check out this pinned post\\non Piazza and if you want to work on any of these projects,\\nthen feel free to contact the project mentors\\ndirectly about these things. Aditionally we posted office\\nhours on the course website, this is a Google calendar, so\\nthis is something that people have been asking about\\nand now it's up there. The final administrative\\nnote is about Google Cloud, as a reminder, because we're\\nsupported by Google Cloud in this class, we're able to\\ngive each of you an additional $100 credit for Google Cloud\\nto work on your assignments and projects, and the exact\\ndetails of how to redeem that credit will go out later\\ntoday, most likely on Piazza. So if there's, I guess if\\nthere's no questions about administrative stuff then we'll\\nmove on to course content. Okay cool. So recall from last time in lecture two, we were really talking about\\nthe challenges of recognition and trying to hone in on this idea of a data-driven approach. We talked about this idea\\nof image classification, talked about why it's hard,\\nthere's this semantic gap between the giant grid of\\nnumbers that the computer sees and the actual image that you see. We talked about various\\nchallenges regarding this around illumination,\\ndeformation, et cetera, and why this is actually a\\nreally, really hard problem even though it's super\\neasy for people to do with their human eyes\\nand human visual system. Then also recall last time\\nwe talked about the k-nearest neighbor classifier as kind\\nof a simple introduction to this whole data-driven mindset. We talked about the CIFAR-10\\ndata set where you can see an example of these images\\non the upper left here, where CIFAR-10 gives you\\nthese 10 different categories, airplane, automobile, whatnot, and we talked about how the\\nk-nearest neighbor classifier can be used to learn decision boundaries to separate these data points into classes based on the training data. This also led us to a\\ndiscussion of the idea of cross validation and setting\\nhyper parameters by dividing your data into train,\\nvalidation and test sets. Then also recall last time\\nwe talked about linear classification as the first\\nsort of building block as we move toward neural networks. Recall that the linear\\nclassifier is an example of a parametric classifier\\nwhere all of our knowledge about the training data gets summarized into this parameter matrix W that is set during the process of training. And this linear classifier\\nrecall is super simple, where we're going to take\\nthe image and stretch it out into a long vector. So here the image is x and\\nthen we take that image which might be 32 by 32 by\\n3 pixels, stretch it out into a long column vector of 32 times 32 times 3 entries, where the 32 and 32 are\\nthe height and width, and the 3 give you\\nthe three color channels, red, green, blue. Then there exists some parameter matrix, W which will take this long column vector representing the image\\npixels, and convert this and give you 10 numbers giving scores for each of the 10 classes\\nin the case of CIFAR-10. Where we kind of had this interpretation where larger values of those scores, so a larger value for the cat\\nclass means the classifier thinks that the cat is\\nmore likely for that image, and lower values for\\nmaybe the dog or car class indicate lower probabilities\\nof those classes being present in the image. Also, so I think this point\\nwas a little bit unclear last time that linear classification\\nhas this interpretation as learning templates per class, where if you look at the\\ndiagram on the lower left, you think that, so for\\nevery pixel in the image, and for every one of our 10 classes, there exists some entry in this matrix W, telling us how much does that\\npixel influence that class. So that means that each of\\nthese rows in the matrix W ends up corresponding to\\na template for the class. And if we take those rows and unravel, so each of those rows again corresponds to a weighting between the values of, between the pixel values of\\nthe image and that class, so if we take that row and\\nunravel it back into an image, then we can visualize the\\nlearned template for each of these classes. We also had this interpretation\\nof linear classification as learning linear decision\\nboundaries between pixels in some high dimensional\\nspace where the dimensions of the space correspond\\nto the values of the pixel intensity values of the image. So this is kind of where\\nwe left off last time. And so where we kind of\\nstopped, where we ended up last time is we got this idea\\nof a linear classifier, and we didn't talk about how\\nto actually choose the W. How to actually use the training data to determine which value\\nof W should be best. So kind of where we stopped off at is that for some setting\\nof W, we can use this W to come up with 10 with our\\nclass scores for any image. So and some of these class\\nscores might be better or worse. So here in this simple example, we've shown maybe just a\\ntraining data set of three images along with the 10 class scores\\npredicted for some value of W for those images. And you can see that some\\nof these scores are better or worse than others. So for example in the image\\non the left, if you look up, it's actually a cat because you're a human and you can tell these things, but if we look at the\\nassigned probabilities, cat, well not probabilities but scores, then the classifier maybe\\nfor this setting of W gave the cat class a score\\nof 2.9 for this image, whereas the frog class gave 3.78. So maybe the classifier\\nis not doing not so good on this image, that's bad,\\nwe wanted the true class to be actually the highest class score, whereas for some of these\\nother examples, like the car for example, you see\\nthat the automobile class has a score of six which is much higher than any of the others, so that's good. And the frog, the predicted\\nscores are maybe negative four, which is much lower\\nthan all the other ones, so that's actually bad. So this is kind of a hand wavy approach, just kind of looking at\\nthe scores and eyeballing which ones are good\\nand which ones are bad. But to actually write\\nalgorithms about these things and to actually to determine\\nautomatically which W will be best, we need some\\nway to quantify the badness of any particular W. And that's this function\\nthat takes in a W, looks at the scores and then\\ntells us how bad quantitatively is that W, is something that\\nwe'll call a loss function. And in this lecture we'll\\nsee a couple examples of different loss functions\\nthat you can use for this image classification problem. So then once we've got this\\nidea of a loss function, this allows us to quantify\\nfor any given value of W, how good or bad is it? But then we actually need to find and come up with an efficient procedure for searching through the\\nspace of all possible Ws and actually come up with\\nwhat is the correct value of W that is the least bad, and this process will be\\nan optimization procedure and we'll talk more about\\nthat in this lecture. So I'm going to shrink\\nthis example a little bit because 10 classes is\\na little bit unwieldy. So we'll kind of work with\\nthis tiny toy data set of three examples and\\nthree classes going forward in this lecture. So again, in this example, the\\ncat is maybe not so correctly classified, the car is correctly\\nclassified, and the frog, this setting of W got this\\nfrog image totally wrong, because the frog score is\\nmuch lower than others. So to formalize this a little\\nbit, usually when we talk about a loss function, we imagine that we have some training\\ndata set of xs and ys, usually N examples of these\\nwhere the xs are the inputs to the algorithm in the\\nimage classification case, the xs would be the actually\\npixel values of your images, and the ys will be the things\\nyou want your algorithm to predict, we usually call\\nthese the labels or the targets. So in the case of image classification, remember we're trying\\nto categorize each image for CIFAR-10 to one of 10 categories, so the label y here will be an integer between one and 10 or\\nmaybe between zero and nine depending on what programming\\nlanguage you're using, but it'll be an integer telling you what is the correct category\\nfor each one of those images x. And now our loss function\\nwill denote L_i to denote the, so then we have this prediction function x which takes in our example\\nx and our weight matrix W and makes some prediction for y, in the case of image classification these will be our 10 numbers. Then we'll define some loss function L_i which will take in the predicted scores coming out of the function f together with the true target or label Y and give us some quantitative\\nvalue for how bad those predictions are for\\nthat training example. And now the final loss L will\\nbe the average of these losses summed over the entire data\\nset over each of the N examples in our data set. So this is actually a\\nvery general formulation, and actually extends even\\nbeyond image classification. Kind of as we move forward\\nand see other tasks, other examples of tasks and deep learning, the kind of generic setup\\nis that for any task you have some xs and ys\\nand you want to write down some loss function that\\nquantifies exactly how happy you are with your particular\\nparameter settings W and then you'll eventually\\nsearch over the space of W to find the W that minimizes\\nthe loss on your training data. So as a first example of\\na concrete loss function that is a nice thing to work\\nwith in image classification, we'll talk about the multi-class SVM loss. You may have seen the binary\\nSVM, our support vector machine in CS 229 and the multiclass SVM is a generalization of that\\nto handle multiple classes. In the binary SVM case as\\nyou may have seen in 229, you only had two classes, each example x was going to be classified\\nas either positive or negative example, but now we have 10 categories,\\nso we need to generalize this notion to handle multiple classes. So this loss function has kind\\nof a funny functional form, so we'll walk through it in a bit more, in quite a bit of detail over\\nthe next couple of slides. But what this is saying\\nis that the loss L_i for any individual example,\\nthe way we'll compute it is we're going to perform a sum\\nover all of the categories, Y, except for the true category, Y_i, so we're going to sum over\\nall the incorrect categories, and then we're going to compare the score of the correct category, and the score of the incorrect category, and now if the score\\nfor the correct category is greater than the score\\nof the incorrect category, greater than the incorrect\\nscore by some safety margin that we set to one, if that's\\nthe case that means that the true score is much, or the\\nscore for the true category is if it's much larger than\\nany of the false categories, then we'll get a loss of zero. And we'll sum this up over all\\nof the incorrect categories for our image and this\\nwill give us our final loss for this one example in the data set. And again we'll take\\nthe average of this loss over the whole training data set. So this kind of like if then\\nstatement, like if the true class score is much\\nlarger than the others, this kind of if then\\nformulation we often compactify into this single max of zero\\nS_j minus S_Yi plus one thing, but I always find that notation\\na little bit confusing, and it always helps me to write it out in this\\nsort of case based notation to figure out exactly\\nwhat the two cases are and what's going on. And by the way, this\\nstyle of loss function where we take max of zero\\nand some other quantity is often referred to as\\nsome type of a hinge loss, and this name comes from\\nthe shape of the graph when you go and plot it, so here the x axis corresponds to the S_Yi, that is the score of the\\ntrue class for some training example, and now the y axis is the loss, and you can see that as the\\nscore for the true category for this example increases, then the loss will go down linearly until we get to above this safety margin, after which the loss will be zero because we've already correctly\\nclassified this example. So let's, oh, question? - [Student] Sorry, in terms of notation what is S underscore Yi? Is that your right score? - Yeah, so the question is in terms of notation,\\nwhat is S and what is SYI in particular, so the Ss\\nare the predicted scores for the classes that are\\ncoming out of the classifier. So if one is the cat class and\\ntwo is the dog class then S1 and S2 would be the cat and\\ndog scores respectively. And remember we said that Yi\\nwas the category of the ground truth label for the example\\nwhich is some integer. So then S sub Y sub i, sorry\\nfor the double subscript, that corresponds to the\\nscore of the true class for the i-th example in the training set. Question? - [Student] So what\\nexactly is this computing? - Yeah the question is what\\nexactly is this computing here? It's a little bit funny, I\\nthink it will become more clear when we walk through an explicit\\nexample, but in some sense what this loss is saying is\\nthat we are happy if the true score is much higher than\\nall the other scores. It needs to be higher\\nthan all the other scores by some safety margin,\\nand if the true score is not high enough, greater\\nthan any of the other scores, then we will incur some\\nloss and that would be bad. So this might make a little bit more sense if we walk through an explicit example for this tiny three example data set. So here remember I've sort\\nof removed the case space notation and just switching\\nback to the zero one notation, and now if we look at, if we think about computing\\nthis multi-class SVM loss for just this first training example on the left, then remember\\nwe're going to loop over all of the incorrect\\nclasses, so for this example, cat is the correct class, so\\nwe're going to loop over the car and frog classes, and now for\\ncar, we're going to compare the, we're going to look at the car\\nscore, 5.1, minus the cat score, 3.2 plus one, when we're\\ncomparing cat and car we expect to incur some loss here because\\nthe car score is greater than the cat score which is bad. So for this one class,\\nfor this one example, we'll incur a loss of 2.9, and then when we go and\\ncompare the cat score and the frog score we see that cat is 3.2, frog is minus 1.7, so cat is more than one greater than frog, which means that between these two classes we incur zero loss. So then the multiclass SVM\\nloss for this training example will be the sum of the losses\\nacross each of these pairs of classes, which will be\\n2.9 plus zero which is 2.9. Which is sort of saying that\\n2.9 is a quantitative measure of how much our classifier screwed up on this one training example. And then if we repeat this procedure for this next car image, then\\nagain the true class is car, so we're going to iterate\\nover all the other categories when we compare the car and the cat score, we see that car is more\\nthan one greater than cat so we get no loss here. When we compare car and frog, we again see that the car score is more\\nthan one greater than frog, so we get again no loss\\nhere, and our total loss for this training example is zero. And now I think you hopefully\\nget the picture by now, but, if you go look at frog, now\\nfrog, we again compare frog and cat, incur quite a lot of\\nloss because the frog score is very low, compare frog\\nand car, incur a lot of loss because the score is very low,\\nand then our loss for this example is 12.9. And then our final loss\\nfor the entire data set is the average of these losses across the different examples, so when you sum those out\\nit comes to about 5.3. So then it's sort of, this\\nis our quantitative measure that our classifier is\\n5.3 bad on this data set. Is there a question? - [Student] How do you\\nchoose the plus one? - Yeah, the question is how\\ndo you choose the plus one? That's actually a really great question, it seems like kind of an\\narbitrary choice here, it's the only constant that\\nappears in the loss function and that seems to offend\\nyour aesthetic sensibilities a bit maybe. But it turns out that this is somewhat of an arbitrary choice,\\nbecause we don't actually care about the absolute values of the scores in this loss function, we only care about the relative differences\\nbetween the scores. We only care that the correct score is much greater than the incorrect scores. So in fact if you imagine\\nscaling up your whole W up or down, then it kind\\nof rescales all the scores correspondingly and if you kind\\nof work through the details and there's a detailed derivation\\nof this in the course notes online, you find this choice\\nof one actually doesn't matter. That this free parameter\\nof one kind of washes out and is canceled with this scale, like the overall setting\\nof the scale in W. And again, check the course\\nnotes for a bit more detail on that. So then I think it's\\nkind of useful to think about a couple different\\nquestions to try to understand intuitively what this loss is doing. So the first question is what's\\ngoing to happen to the loss if we change the scores of the\\ncar image just a little bit? Any ideas? Everyone's too scared to ask a question? Answer? [student speaking faintly] - Yeah, so the answer is\\nthat if we jiggle the scores for this car image a little\\nbit, the loss will not change. So the SVM loss, remember,\\nthe only thing it cares about is getting the correct\\nscore to be greater than one more than the incorrect\\nscores, but in this case, the car score is already quite\\na bit larger than the others, so if the scores for this\\nclass changed for this example changed just a little\\nbit, this margin of one will still be retained and\\nthe loss will not change, we'll still get zero loss. The next question, what's\\nthe min and max possible loss for SVM? [student speaking faintly] Oh I hear some murmurs. So the minimum loss is zero,\\nbecause if you can imagine that across all the classes, if\\nour correct score was much larger then we'll incur zero\\nloss across all the classes and it will be zero, and if you think back to this\\nhinge loss plot that we had, then you can see that if the correct score goes very, very negative,\\nthen we could incur potentially infinite loss. So the min is zero and\\nthe max is infinity. Another question, sort of when\\nyou initialize these things and start training from scratch, usually you kind of initialize W with some small random values,\\nas a result your scores tend to be sort of small\\nuniform random values at the beginning of training. And then the question is\\nthat if all of your Ss, if all of the scores\\nare approximately zero and approximately equal, then what kind of loss do you expect when you're using multiclass SVM? - [Student] Number of classes minus one. - Yeah, so the answer is\\nnumber of classes minus one, because remember that\\nif we're looping over all of the incorrect classes,\\nso we're looping over C minus one classes, within\\neach of those classes the two Ss will be about the same, so we'll get a loss of one because of the margin and\\nwe'll get C minus one. So this is actually kind\\nof useful because when you, this is a useful debugging strategy when you're using these things, that when you start off training, you should think about what\\nyou expect your loss to be, and if the loss you actually\\nsee at the start of training at that first iteration is\\nnot equal to C minus one in this case, that means you probably have\\na bug and you should go check your code, so this is actually\\nkind of a useful thing to be checking in practice. Another question, what happens\\nif, so I said we're summing an SVM over the incorrect\\nclasses, what happens if the sum is also over the correct class if we just go over everything? - [Student] The loss increases by one. - Yeah, so the answer is that\\nthe loss increases by one. And I think the reason\\nthat we do this in practice is because normally loss of\\nzero is kind of, has this nice interpretation that\\nyou're not losing at all, so that's nice, so I think your answers wouldn't really change, you would end up finding\\nthe same classifier if you actually looped\\nover all the categories, but if just by conventions\\nwe omit the correct class so that our minimum loss is zero. So another question, what if we used mean instead of sum here? - [Student] Doesn't change. - Yeah, the answer is\\nthat it doesn't change. So the number of classes is\\ngoing to be fixed ahead of time when we select our data set,\\nso that's just rescaling the whole loss function by a constant, so it doesn't really matter,\\nit'll sort of wash out with all the other scale things because we don't actually\\ncare about the true values of the scores, or the\\ntrue value of the loss for that matter. So now here's another\\nexample, what if we change this loss formulation and we\\nactually added a square term on top of this max? Would this end up being the same problem or would this be a different\\nclassification algorithm? - [Student] Different. - Yes, this would be different. So here the idea is that\\nwe're kind of changing the trade-offs between good and badness in kind of a nonlinear way, so this would end up actually computing a different loss function. This idea of a squared hinge\\nloss actually does get used sometimes in practice, so\\nthat's kind of another trick to have in your bag when you're making up your own loss functions\\nfor your own problems. So now you'll end up,\\noh, was there a question? - [Student] Why would\\nyou use a squared loss instead of a non-squared loss? - Yeah, so the question is\\nwhy would you ever consider using a squared loss instead\\nof a non-squared loss? And the whole point of a loss function is to kind of quantify how\\nbad are different mistakes. And if the classifier is making\\ndifferent sorts of mistakes, how do we weigh off the\\ndifferent trade-offs between different types\\nof mistakes the classifier might make? So if you're using a squared loss, that sort of says that things\\nthat are very, very bad are now going to be squared bad so that's like really, really bad, like we don't want anything that's totally\\ncatastrophically misclassified, whereas if you're using this hinge loss, we don't actually care between\\nbeing a little bit wrong and being a lot wrong, being\\na lot wrong kind of like, if an example is a lot\\nwrong, and we increase it and make it a little bit less wrong, that's kind of the same\\ngoodness as an example which was only a little bit\\nwrong and then increasing it to be a little bit more right. So that's a little bit hand wavy, but this idea of using\\na linear versus a square is a way to quantify how much we care about different categories of errors. And this is definitely something\\nthat you should think about when you're actually applying\\nthese things in practice, because the loss function is the way that you tell your algorithm\\nwhat types of errors you care about and what types of errors it should trade off against. So that's actually super\\nimportant in practice depending on your application. So here's just a little snippet\\nof sort of vectorized code in numpy, and you'll end up implementing something like this for\\nthe first assignment, but this kind of gives you\\nthe sense that this sum is actually like pretty easy to implement in numpy, it\\nonly takes a couple lines of vectorized code. And you can see in practice,\\nlike one nice trick is that we can actually go in\\nhere and zero out the margins corresponding to the correct class, and that makes it easy to then just, that's sort of one nice\\nvectorized trick to skip, iterate over all but one class. You just kind of zero out\\nthe one you want to skip and then compute the sum\\nanyway, so that's a nice trick you might consider\\nusing on the assignment. So now, another question\\nabout this loss function. Suppose that you were\\nlucky enough to find a W that has loss of zero,\\nyou're not losing at all, you're totally winning, this loss function is crushing it, but then there's a\\nquestion, is this W unique or were there other Ws that could also have achieved zero loss? - [Student] There are other Ws. - Answer, yeah, so there\\nare definitely other Ws. And in particular, because\\nwe talked a little bit about this thing of scaling\\nthe whole problem up or down depending on W, so you\\ncould actually take W multiplied by two and this\\ndoubled W (Is it quad U now? I don't know.) [laughing] This would also achieve zero loss. So as a concrete example of this, you can go back to your favorite example and maybe work through the numbers a little bit later, but if you're taking W and we double W, then the margins between the\\ncorrect and incorrect scores will also double. So that means that if all these margins were already greater than\\none, and we doubled them, they're still going to\\nbe greater than one, so you'll still have zero loss. And this is kind of interesting, because if our loss function is the way that we tell our\\nclassifier which W we want and which W we care about, this is a little bit weird, now there's this inconsistency and how is the classifier to choose between these different versions of W that all achieve zero loss? And that's because what we've done here is written down only a\\nloss in terms of the data, and we've only told our classifier that it should try to find the W that fits the training data. But really in practice,\\nwe don't actually care that much about fitting the training data, the whole point of machine learning is that we use the training\\ndata to find some classifier and then we'll apply\\nthat thing on test data. So we don't really care about\\nthe training data performance, we really care about the performance of this classifier on test data. So as a result, if the only thing we're telling our classifier to do is fit the training data, then we can lead ourselves into some of these weird\\nsituations sometimes, where the classifier might\\nhave unintuitive behavior. So a concrete, canonical example\\nof this sort of thing, by the way, this is not\\nlinear classification anymore, this is a little bit of a more general machine learning concept, is that suppose we have this\\ndata set of blue points, and we're going to fit some\\ncurve to the training data, the blue points, then if the only thing we've\\ntold our classifier to do is to try and fit the training data, it might go in and have very wiggly curves to try to perfectly classify all of the training data points. But this is bad, because\\nwe don't actually care about this performance, we care about the\\nperformance on the test data. So now if we have some new data come in that sort of follows the same trend, then this very wiggly blue line is going to be totally wrong. And in fact, what we\\nprobably would have preferred the classifier to do was maybe predict this straight green line, rather than this very complex wiggly line to perfectly fit all the training data. And this is a core fundamental problem in machine learning, and the way we usually solve it, is this concept of regularization. So here we're going to\\nadd an additional term to the loss function. In addition to the data loss, which will tell our\\nclassifier that it should fit the training data,\\nwe'll also typically add another term to the loss function called a regularization term, which encourages the model\\nto somehow pick a simpler W, where the concept of simple kind of depends on the task and the model. There's this whole idea of Occam's Razor, which is this fundamental\\nidea in scientific discovery more broadly, which is that\\nif you have many different competing hypotheses, that could explain your observations, you should generally\\nprefer the simpler one, because that's the explanation\\nthat is more likely to generalize to new\\nobservations in the future. And the way we\\noperationalize this intuition in machine learning is\\ntypically through some explicit regularization penalty that's often written down as R. So then your standard loss function usually has these two terms, a data loss and a regularization loss, and there's some\\nhyper-parameter here, lambda, that trades off between the two. And we talked about hyper-parameters and cross-validation in the last lecture, so this regularization\\nhyper-parameter lambda will be one of the more important ones that you'll need to tune when training these models in practice. Question? - [Student] What does that lambda R W term have to do with [speaking faintly]. - Yeah, so the question is, what's the connection\\nbetween this lambda R W term and actually forcing this wiggly line to become a straight green line? I didn't want to go through\\nthe derivation on this because I thought it would\\nlead us too far astray, but you can imagine, maybe you're doing a regression problem, in terms of different\\npolynomial basis functions, and if you're adding\\nthis regression penalty, maybe the model has access to polynomials of very high degree, but\\nthrough this regression term you could encourage the\\nmodel to prefer polynomials of lower degree, if they\\nfit the data properly, or if they fit the data relatively well. So you could imagine\\nthere's two ways to do this, either you can constrain your model class to just not contain the more powerful, more complex models, or you\\ncan add this soft penalty where the model still has\\naccess to more complex models, maybe high degree\\npolynomials in this case, but you add this soft constraint saying that if you want to\\nuse these more complex models, you need to overcome this penalty for using their complexity. So that's the connection here, that is not quite linear classification, this is the picture that\\nmany people have in mind when they think about\\nregularization at least. So there's actually a\\nlot of different types of regularization that\\nget used in practice. The most common one is\\nprobably L2 regularization, or weight decay. But there's a lot of other\\nones that you might see. This L2 regularization is\\njust the euclidean norm of this weight vector W, or sometimes the squared norm. Or sometimes half the squared norm because it makes your derivatives work out a little bit nicer. But the idea of L2 regularization is you're just penalizing\\nthe euclidean norm of this weight vector. You might also sometimes\\nsee L1 regularization, where we're penalizing the\\nL1 norm of the weight vector, and the L1 regularization\\nhas some nice properties like encouraging sparsity\\nin this matrix W. Some other things you might see would be this elastic net regularization, which is some combination of L1 and L2. You sometimes see max norm regularization, penalizing the max norm\\nrather than the L1 or L2 norm. But these sorts of regularizations are things that you see\\nnot just in deep learning, but across many areas of machine learning and even optimization more broadly. In some later lectures, we'll also see some types of regularization\\nthat are more specific to deep learning. For example dropout, we'll\\nsee in a couple lectures, or batch normalization, stochastic depth, these things get kind of\\ncrazy in recent years. But the whole idea of regularization is just any thing that\\nyou do to your model, that sort of penalizes somehow\\nthe complexity of the model, rather than explicitly trying\\nto fit the training data. Question? [student speaking faintly] Yeah, so the question is, how does the L2 regularization\\nmeasure the complexity of the model? Thankfully we have an\\nexample of that right here, maybe we can walk through. So here we maybe have\\nsome training example, x, and there's two different\\nWs that we're considering. So x is just this vector of four ones, and we're considering these\\ntwo different possibilities for W. One is a one in the\\nfirst, one is a single one and three zeros, and the other has this 0.25 spread across the four different entries. And now, when we're doing\\nlinear classification, we're really taking dot products between our x and our W. So in terms of linear classification, these two Ws are the same, because they give the same result when dot producted with x. But now the question is, if you look at these two examples, which one would L2 regression prefer? Yeah, so L2 regression would prefer W2, because it has a smaller norm. So the answer is that the L2 regression measures complexity of the classifier in this relatively coarse way, where the idea is that, remember the Ws in linear classification had this interpretation of how much does this value of the vector x correspond to this output class? So L2 regularization is saying that it prefers to spread that influence across all the different values in x. Maybe this might be more robust, in case you come up with xs that vary, then our decisions are spread out and depend on the entire x vector, rather than depending\\nonly on certain elements of the x vector. And by the way, L1 regularization has this opposite interpretation. So actually if we were\\nusing L1 regularization, then we would actually prefer W1 over W2, because L1 regularization\\nhas this different notion of complexity, saying that\\nmaybe the model is less complex, maybe we measure model\\ncomplexity by the number of zeros in the weight vector, so the question of how\\ndo we measure complexity and how does L2 measure complexity? They're kind of problem dependent. And you have to think about\\nfor your particular setup, for your particular model and data, how do you think that\\ncomplexity should be measured on this task? Question? - [Student] So why would L1 prefer W1? Don't they sum to the same one? - Oh yes, you're right. So in this case, L1 is actually the same between these two. But you could construct\\na similar example to this where W1 would be preferred\\nby L1 regularization. I guess the general intuition behind L1 is that it generally\\nprefers sparse solutions, that it drives all your\\nentries of W to zero for most of the entries,\\nexcept for a couple where it's allowed to deviate from zero. The way of measuring complexity for L1 is maybe the number of non-zero entries, and then for L2, it thinks\\nthat things that spread the W across all the values are less complex. So it depends on your data,\\ndepends on your problem. Oh and by the way, if\\nyou're a hardcore Bayesian, then using L2 regularization\\nhas this nice interpretation of MAP inference under a Gaussian prior on the parameter vector. I think there was a\\nhomework problem about that in 229, but we won't talk about that for the rest of the quarter. That's sort of my long, deep dive into the multi-class SVM loss. Question? - [Student] Yeah, so I'm still confused about what the kind of stuff I need to do when the linear versus polynomial thing, because the use of this loss function isn't going to change the\\nfact that you're just doing, you're looking at a\\nlinear classifier, right? - Yeah, so the question is that, adding a regularization is not going to change\\nthe hypothesis class. This is not going to change us\\naway from a linear classifier. The idea is that maybe this example of this polynomial regression is definitely not linear regression. That could be seen as linear regression on top of a polynomial\\nexpansion of the input, and in which case, this\\nregression sort of says that you're not allowed\\nto use as many polynomial coefficients as maybe you should have. Right, so you can imagine this is like, when you're doing polynomial regression, you can write out a polynomial as f of x equals A zero plus A one\\nx plus A two x squared plus A three x whatever, in that case your parameters, your Ws, would be these As, in which case, penalizing the W could force it towards lower degree polynomials. Except in the case of\\npolynomial regression, you don't actually want to parameterize in terms of As, there's\\nsome other paramterization that you want to use, but that's the general idea, that you're sort of penalizing\\nthe parameters of the model to force it towards the simpler hypotheses within your hypothesis class. And maybe we can take this offline if that's still a bit confusing. So then we've sort of seen\\nthis multi-class SVM loss, and just by the way as a side note, this is one extension or\\ngeneralization of the SVM loss to multiple classes, there's actually a couple\\ndifferent formulations that you can see around in literature, but I mean, my intuition is\\nthat they all tend to work similarly in practice, at least in the context of deep learning. So we'll stick with this\\none particular formulation of the multi-class SVM loss in this class. But of course there's many\\ndifferent loss functions you might imagine. And another really popular choice, in addition to the multi-class SVM loss, another really popular\\nchoice in deep learning is this multinomial logistic regression, or a softmax loss. And this one is probably\\nactually a bit more common in the context of deep learning, but I decided to present\\nthis second for some reason. So remember in the context\\nof the multi-class SVM loss, we didn't actually have an interpretation for those scores. Remember, when we're\\ndoing some classification, our model F, spits our these 10 numbers, which are our scores for the classes, and for the multi-class SVM, we didn't actually give much\\ninterpretation to those scores. We just said that we want the true score, the score of the correct class to be greater than the incorrect classes, and beyond that we don't really\\nsay what those scores mean. But now, for the multinomial\\nlogistic regression loss function, we actually\\nwill endow those scores with some additional meaning. And in particular we're\\ngoing to use those scores to compute a probability distribution over our classes. So we use this so-called softmax function where we take all of our scores, we exponentiate them so that\\nnow they become positive, then we re-normalize them by\\nthe sum of those exponents so now after we send our scores through this softmax function, now we end up with this\\nprobability distribution, where now we have\\nprobabilities over our classes, where each probability\\nis between zero and one, and the sum of probabilities\\nacross all classes sum to one. And now the interpretation\\nis that we want, there's this computed\\nprobability distribution that's implied by our scores, and we want to compare\\nthis with the target or true probability distribution. So if we know that the thing is a cat, then the target probability distribution would put all of the\\nprobability mass on cat, so we would have probability\\nof cat equals one, and zero probability for\\nall the other classes. So now what we want to do is encourage our computed probability distribution that's coming out of this softmax function to match this target\\nprobability distribution that has all the mass\\non the correct class. And the way that we do this, I mean, you can do this\\nequation in many ways, you can do this as a KL divergence between the target and the computed probability distribution, you can do this as a\\nmaximum likelihood estimate, but at the end of the day, what we really want is\\nthat the probability of the true class is\\nhigh and as close to one. So then our loss will\\nnow be the negative log of the probability of the true class. This is confusing 'cause\\nwe're putting this through multiple different things, but remember we wanted the probability to be close to one, so now log is a monotonic\\nfunction, it goes like this, and it turns out mathematically, it's easier to maximize log than it is to maximize\\nthe raw probability, so we stick with log. And now log is monotonic, so if we maximize log P of correct class, that means we want that to be high, but loss functions measure\\nbadness not goodness so we need to put in the minus one to make it go the right way. So now our loss function for SVM is going to be the minus\\nlog of the probability of the true class. Yeah, so that's the summary here, is that we take our scores,\\nwe run through the softmax, and now our loss is this\\nminus log of the probability of the true class. Okay, so then if you look\\nat what this looks like on a concrete example, then we go back to our\\nfavorite beautiful cat with our three examples and\\nwe've got these three scores that are coming out of\\nour linear classifier, and these scores are exactly\\nthe way that they were in the context of the SVM loss. But now, rather than taking these scores and putting them directly\\ninto our loss function, we're going to take them\\nall and exponentiate them so that they're all positive, and then we'll normalize them to make sure that they all sum to one. And now our loss will be the minus log of the true class score. So that's the softmax loss, also called multinomial\\nlogistic regression. So now we asked several questions to try to gain intuition about\\nthe multi-class SVM loss, and it's useful to think about\\nsome of the same questions to contrast with the softmax loss. So then the question is, what's the min and max\\nvalue of the softmax loss? Okay, maybe not so sure, there's too many logs and sums and stuff going on in here. So the answer is that the min loss is zero and the max loss is infinity. And the way that you can see this, the probability distribution that we want is one on the correct class,\\nzero on the incorrect classes, the way that we do that is, so if that were the case, then this thing inside the\\nlog would end up being one, because it's the log\\nprobability of the true class, then log of one is zero, minus\\nlog of one is still zero. So that means that if we\\ngot the thing totally right, then our loss would be zero. But by the way, in order to\\nget the thing totally right, what would our scores have to look like? Murmuring, murmuring. So the scores would actually\\nhave to go quite extreme, like towards infinity. So because we actually\\nhave this exponentiation, this normalization, the only way we can actually get a\\nprobability distribution of one and zero, is actually\\nputting an infinite score for the correct class,\\nand minus infinity score for all the incorrect classes. And computers don't do\\nso well with infinities, so in practice, you'll\\nnever get to zero loss on this thing with finite precision. But you still have this interpretation that zero is the theoretical\\nminimum loss here. And the maximum loss is unbounded. So suppose that if we\\nhad zero probability mass on the correct class, then\\nyou would have minus log of zero, log of zero is minus infinity, so minus log of zero\\nwould be plus infinity, so that's really bad. But again, you'll never really get here because the only way you can\\nactually get this probability to be zero, is if e to the\\ncorrect class score is zero, and that can only happen\\nif that correct class score is minus infinity. So again, you'll never\\nactually get to these minimum, maximum values with finite precision. So then, remember we had this debugging, sanity check question in the\\ncontext of the multi-class SVM, and we can ask the same for the softmax. If all the Ss are small and about zero, then what is the loss here? Yeah, answer? - [Student] Minus log one over C. - So minus log of one over C? I think that's, yeah, so then it'd be minus log of one over C, because log can flip the thing so then it's just log of C. Yeah, so it's just log of C. And again, this is a nice debugging thing, if you're training a model\\nwith this softmax loss, you should check at the first iteration. If it's not log C, then\\nsomething's gone wrong. So then we can compare and\\ncontrast these two loss functions a bit. In terms of linear classification, this setup looks the same. We've got this W matrix\\nthat gets multiplied against our input to produce\\nthis specter of scores, and now the difference\\nbetween the two loss functions is how we choose to interpret those scores to quantitatively measure\\nthe badness afterwards. So for SVM, we were going to\\ngo in and look at the margins between the scores of the correct class and the scores of the incorrect class, whereas for this softmax\\nor cross-entropy loss, we're going to go and compute\\na probability distribution and then look at the minus log probability of the correct class. So sometimes if you look at, in terms of, nevermind,\\nI'll skip that point. [laughing] So another question that's interesting when contrasting these two\\nloss functions is thinking, suppose that I've got this example point, and if you change its scores, so assume that we've got\\nthree scores for this, ignore the part on the bottom. But remember if we go back to this example where in the multi-class SVM loss, when we had the car, and the\\ncar score was much better than all the incorrect classes, then jiggling the scores\\nfor that car image didn't change the\\nmulti-class SVM loss at all, because the only thing that the SVM loss cared about was getting that correct score to be greater than a margin\\nabove the incorrect scores. But now the softmax loss\\nis actually quite different in this respect. The softmax loss actually\\nalways wants to drive that probability mass all the way to one. So even if you're giving very high score to the correct class, and very low score to all the incorrect classes, softmax will want you to pile\\nmore and more probability mass on the correct class, and\\ncontinue to push the score of that correct class up towards infinity, and the score of the incorrect classes down towards minus infinity. So that's the interesting difference between these two loss\\nfunctions in practice. That SVM, it'll get this\\ndata point over the bar to be correctly classified\\nand then just give up, it doesn't care about\\nthat data point any more. Whereas softmax will just always\\ntry to continually improve every single data point\\nto get better and better and better and better. So that's an interesting difference between these two functions. In practice, I think it tends\\nnot to make a huge difference which one you choose, they tend to perform pretty similarly across, at least a lot of deep\\nlearning applications. But it is very useful to keep\\nsome of these differences in mind. Yeah, so to recap where\\nwe've come to from here, is that we've got some\\ndata set of xs and ys, we use our linear classifier\\nto get some score function, to compute our scores\\nS, from our inputs, x, and then we'll use a loss function, maybe softmax or SVM or\\nsome other loss function to compute how quantitatively\\nbad were our predictions compared to this ground true targets, y. And then we'll often\\naugment this loss function with a regularization term, that tries to trade off between\\nfitting the training data and preferring simpler models. So this is a pretty generic overview of a lot of what we call\\nsupervised learning, and what we'll see in deep\\nlearning as we move forward, is that generally you'll want\\nto specify some function, f, that could be very complex in structure, specify some loss function that determines how well your algorithm is doing, given any value of the parameters, some regularization term for how to penalize model complexity and then you combine these things together and try to find the W that minimizes this final loss function. But then the question is, how do we actually go about doing that? How do we actually find this\\nW that minimizes the loss? And that leads us to the\\ntopic of optimization. So when we're doing optimization, I usually think of things\\nin terms of walking around some large valley. So the idea is that you're\\nwalking around this large valley with different mountains\\nand valleys and streams and stuff, and every\\npoint on this landscape corresponds to some setting\\nof the parameters W. And you're this little guy who's\\nwalking around this valley, and you're trying to find, and the height of each of these points, sorry, is equal to the loss\\nincurred by that setting of W. And now your job as this little man walking around this landscape, you need to somehow find\\nthe bottom of this valley. And this is kind of a\\nhard problem in general. You might think, maybe I'm really smart and I can think really hard\\nabout the analytic properties of my loss function, my\\nregularization all that, maybe I can just write down the minimizer, and that would sort of correspond\\nto magically teleporting all the way to the bottom of this valley. But in practice, once your\\nprediction function, f, and your loss function\\nand your regularizer, once these things get big and complex and using neural networks, there's really not much\\nhope in trying to write down an explicit analytic solution that takes you directly to the minima. So in practice we tend to use various\\ntypes of iterative methods where we start with some solution and then gradually improve it over time. So the very first, stupidest thing that you might imagine is random search, that will just take a bunch of Ws, sampled randomly, and throw\\nthem into our loss function and see how well they do. So spoiler alert, this is\\na really bad algorithm, you probably shouldn't use this, but at least it's one thing\\nyou might imagine trying. And we can actually do this, we can actually try to\\ntrain a linear classifier via random search, for CIFAR-10 and for this there's 10 classes, so random chance is 10%, and if we did some\\nnumber of random trials, we eventually found just\\nthrough sheer dumb luck, some setting of W that\\ngot maybe 15% accuracy. So it's better than random, but state of the art is maybe 95% so we've got a little\\nbit of gap to close here. So again, probably don't\\nuse this in practice, but you might imagine\\nthat this is something you could potentially do. So in practice, maybe a better strategy is actually using some\\nof the local geometry of this landscape. So if you're this little guy who's walking around this landscape, maybe you can't see directly the path down to the bottom of the valley, but what you can do is feel with your foot and figure out what is the local geometry, if I'm standing right here, which way will take me\\na little bit downhill? So you can feel with your feet and feel where is the slope of the ground taking me down a little\\nbit in this direction? And you can take a step in that direction, and then you'll go down a little bit, feel again with your feet to\\nfigure out which way is down, and then repeat over and over again and hope that you'll end up at the bottom of the valley eventually. So this also seems like a\\nrelatively simple algorithm, but actually this one\\ntends to work really well in practice if you get\\nall the details right. So this is generally the\\nstrategy that we'll follow when training these large neural networks and linear classifiers and other things. So then, that was a little hand wavy, so what is slope? If you remember back\\nto your calculus class, then at least in one dimension, the slope is the derivative\\nof this function. So if we've got some\\none-dimensional function, f, that takes in a scalar x,\\nand then outputs the height of some curve, then we\\ncan compute the slope or derivative at any point by imagining, if we take a small step,\\nh, in any direction, take a small step, h, and\\ncompare the difference in the function value over that step and then drag the step size to zero, that will give us the\\nslope of that function at that point. And this generalizes quite naturally to multi-variable functions as well. So in practice, our x\\nis maybe not a scalar but a whole vector, 'cause remember, x\\nmight be a whole vector, so we need to generalize this notion to multi-variable things. And the generalization that\\nwe use of the derivative in the multi-variable\\nsetting is the gradient, so the gradient is a vector\\nof partial derivatives. So the gradient will\\nhave the same shape as x, and each element of the\\ngradient will tell us what is the slope of the function f, if we move in that coordinate direction. And the gradient turns out to have these very nice properties, so the gradient is now a\\nvector of partial derivatives, but it points in the\\ndirection of greatest increase of the function and correspondingly, if you look at the negative\\ngradient direction, that gives you the direction\\nof greatest decrease of the function. And more generally, if you want to know, what is the slope of my\\nlandscape in any direction? Then that's equal to the\\ndot product of the gradient with the unit vector\\ndescribing that direction. So this gradient is super important, because it gives you this\\nlinear, first-order approximation to your function at your current point. So in practice, a lot of deep learning is about computing\\ngradients of your functions and then using those gradients\\nto iteratively update your parameter vector. So one naive way that you might imagine actually evaluating this\\ngradient on a computer, is using the method of finite differences, going back to the limit\\ndefinition of gradient. So here on the left, we\\nimagine that our current W is this parameter vector that maybe gives us some\\ncurrent loss of maybe 1.25 and our goal is to\\ncompute the gradient, dW, which will be a vector\\nof the same shape as W, and each slot in that\\ngradient will tell us how much will the loss\\nchange is we move a tiny, infinitesimal amount in\\nthat coordinate direction. So one thing you might imagine is just computing these\\nfinite differences, that we have our W, we\\nmight try to increment the first element of\\nW by a small value, h, and then re-compute the\\nloss using our loss function and our classifier and all that. And maybe in this setting,\\nif we move a little bit in the first dimension,\\nthen our loss will decrease a little bit from 1.2534 to 1.25322. And then we can use this limit definition to come up with this finite\\ndifferences approximation to the gradient in this first dimension. And now you can imagine\\nrepeating this procedure in the second dimension, where now we take the first dimension, set it back to the original value, and now increment the second\\ndirection by a small step. And again, we compute the loss and use this finite\\ndifferences approximation to compute an approximation\\nto the gradient in the second slot. And now repeat this again for the third, and on and on and on. So this is actually a terrible idea because it's super slow. So you might imagine that\\ncomputing this function, f, might actually be super\\nslow if it's a large, convolutional neural network. And this parameter vector, W, probably will not have 10\\nentries like it does here, it might have tens of millions or even hundreds of millions\\nfor some of these large, complex deep learning models. So in practice, you'll\\nnever want to compute your gradients for your\\nfinite differences, 'cause you'd have to wait\\nfor hundreds of millions potentially of function evaluations to get a single gradient,\\nand that would be super slow and super bad. But thankfully we don't have to do that. Hopefully you took a calculus course at some point in your lives, so you know that thanks to these guys, we can just write down the\\nexpression for our loss and then use the magical\\nhammer of calculus to just write down an expression for what this gradient should be. And this'll be way more efficient than trying to compute it analytically via finite differences. One, it'll be exact, and two, it'll be much faster\\nsince we just need to compute this single expression. So what this would look like is now, if we go back to this\\npicture of our current W, rather than iterating over\\nall the dimensions of W, we'll figure out ahead of time what is the analytic\\nexpression for the gradient, and then just write it down\\nand go directly from the W and compute the dW or\\nthe gradient in one step. And that will be much better in practice. So in summary, this numerical gradient is something that's\\nsimple and makes sense, but you won't really use it in practice. In practice, you'll always\\ntake an analytic gradient and use that when actually performing\\nthese gradient computations. However, one interesting note is that these numeric gradients\\nare actually a very useful debugging tool. Say you've written some code, and you wrote some code\\nthat computes the loss and the gradient of the loss, then how do you debug this thing? How do you make sure that\\nthis analytic expression that you derived and wrote down in code is actually correct? So a really common debugging\\nstrategy for these things is to use the numeric gradient as a way, as sort of a unit test to make sure that your analytic gradient was correct. Again, because this is\\nsuper slow and inexact, then when doing this\\nnumeric gradient checking, as it's called, you'll tend\\nto scale down the parameter of the problem so that it actually runs in a reasonable amount of time. But this ends up being a super\\nuseful debugging strategy when you're writing your\\nown gradient computations. So this is actually very\\ncommonly used in practice, and you'll do this on\\nyour assignments as well. So then once we know how\\nto compute the gradient, then it leads us to this\\nsuper simple algorithm that's like three lines, but\\nturns out to be at the heart of how we train even these very biggest, most complex deep learning algorithms, and that's gradient descent. So gradient descent is\\nfirst we initialize our W as some random thing, then while true, we'll compute our loss and our gradient and then we'll update our weights in the opposite of the gradient direction, 'cause remember that the gradient was pointing in the direction\\nof greatest increase of the function, so minus gradient points in the direction\\nof greatest decrease, so we'll take a small\\nstep in the direction of minus gradient, and\\njust repeat this forever and eventually your network will converge and you'll be very happy, hopefully. But this step size is\\nactually a hyper-parameter, and this tells us that every\\ntime we compute the gradient, how far do we step in that direction. And this step size, also\\nsometimes called a learning rate, is probably one of the\\nsingle most important hyper-parameters that you need to set when you're actually training\\nthese things in practice. Actually for me when I'm\\ntraining these things, trying to figure out this step size or this learning rate, is\\nthe first hyper-parameter that I always check. Things like model size or\\nregularization strength I leave until a little bit later, and getting the learning\\nrate or the step size correct is the first thing that I\\ntry to set at the beginning. So pictorially what this looks like here's a simple example in two dimensions. So here we've got maybe this bowl that's showing our loss function where this red region in the center is this region of low\\nloss we want to get to and these blue and green\\nregions towards the edge are higher loss that we want to avoid. So now we're going to start of our W at some random point in the space, and then we'll compute the\\nnegative gradient direction, which will hopefully\\npoint us in the direction of the minima eventually. And if we repeat this over and over again, we'll hopefully eventually\\nget to the exact minima. And what this looks like in practice is, oh man, we've got this\\nmouse problem again. So what this looks like in practice is that if we repeat this\\nthing over and over again, then we will start off at some point and eventually, taking tiny\\ngradient steps each time, you'll see that the parameter\\nwill arc in toward the center, this region of minima, and that's really what you want, because you want to get to low loss. And by the way, as a bit of a teaser, we saw in the previous slide, this example of very\\nsimple gradient descent, where at every step, we're\\njust stepping in the direction of the gradient. But in practice, over the\\nnext couple of lectures, we'll see that there are\\nslightly fancier step, what they call these update rules, where you can take slightly fancier things to incorporate gradients\\nacross multiple time steps and stuff like that, that tend\\nto work a little bit better in practice and are\\nused much more commonly than this vanilla gradient descent when training these things in practice. And then, as a bit of a preview, we can look at some of these\\nslightly fancier methods on optimizing the same problem. So again, the black will be\\nthis same gradient computation, and these, I forgot which color they are, but these two other curves are using slightly fancier update rules to decide exactly how to\\nuse the gradient information to make our next step. So one of these is gradient\\ndescent with momentum, the other is this Adam optimizer, and we'll see more details about those later in the course. But the idea is that we have\\nthis very basic algorithm called gradient descent, where we use the gradient\\nat every time step to determine where to step next, and there exist different\\nupdate rules which tell us how exactly do we use\\nthat gradient information. But it's all the same basic algorithm of trying to go downhill\\nat every time step. But there's actually\\none more little wrinkle that we should talk about. So remember that we\\ndefined our loss function, we defined a loss that computes how bad is our classifier doing at\\nany single training example, and then we said that our\\nfull loss over the data set was going to be the average loss across the entire training set. But in practice, this N\\ncould be very very large. If we're using the image\\nnet data set for example, that we talked about in the first lecture, then N could be like 1.3 million, so actually computing this loss could be actually very expensive and require computing perhaps\\nmillions of evaluations of this function. So that could be really slow. And actually, because the\\ngradient is a linear operator, when you actually try\\nto compute the gradient of this expression, you see\\nthat the gradient of our loss is now the sum of the\\ngradient of the losses for each of the individual terms. So now if we want to\\ncompute the gradient again, it sort of requires us to iterate over the entire training data set all N of these examples. So if our N was like a million, this would be super super slow, and we would have to wait\\na very very long time before we make any individual update to W. So in practice, we tend to use what is called stochastic\\ngradient descent, where rather than computing\\nthe loss and gradient over the entire training set, instead at every iteration,\\nwe sample some small set of training examples, called a minibatch. Usually this is a power\\nof two by convention, like 32, 64, 128 are common numbers, and then we'll use this small minibatch to compute an estimate of the full sum, and an estimate of the true gradient. And now this is stochastic\\nbecause you can view this as maybe a Monte Carlo\\nestimate of some expectation of the true value. So now this makes our\\nalgorithm slightly fancier, but it's still only four lines. So now it's well true, sample\\nsome random minibatch of data, evaluate your loss and\\ngradient on the minibatch, and now make an update on your parameters based on this estimate of the loss, and this estimate of the gradient. And again, we'll see\\nslightly fancier update rules of exactly how to integrate\\nmultiple gradients over time, but this is the\\nbasic training algorithm that we use for pretty much\\nall deep neural networks in practice. So we have another interactive web demo actually playing around\\nwith linear classifiers, and training these things via\\nstochastic gradient descent, but given how miserable\\nthe web demo was last time, I'm not actually going to open the link. Instead, I'll just play this video. [laughing] But I encourage you to go check this out and play with it online, because it actually helps\\nto build some intuition about linear classifiers and training them via gradient descent. So here you can see on the left, we've got this problem\\nwhere we're categorizing three different classes, and we've got these\\ngreen, blue and red points that are our training samples\\nfrom these three classes. And now we've drawn\\nthe decision boundaries for these classes, which are\\nthe colored background regions, as well as these directions, giving you the direction of\\nincrease for the class scores for each of these three classes. And now if you see, if\\nyou actually go and play with this thing online, you can see that we can\\ngo in and adjust the Ws and changing the values of the Ws will cause these decision\\nboundaries to rotate. If you change the biases,\\nthen the decision boundaries will not rotate, but will\\ninstead move side to side or up and down. Then we can actually make steps that are trying to update this loss, or you can change the step\\nsize with this slider. You can hit this button\\nto actually run the thing. So now with a big step size, we're running gradient descent right now, and these decision boundaries\\nare flipping around and trying to fit the data. So it's doing okay now, but we can actually change\\nthe loss function in real time between these different SVM formulations and the different softmax. And you can see that as you flip between these different\\nformulations of loss functions, it's generally doing the same thing. Our decision regions are\\nmostly in the same place, but exactly how they end\\nup relative to each other and exactly what the trade-offs are between categorizing\\nthese different things changes a little bit. So I really encourage you to go online and play with this thing to\\ntry to get some intuition for what it actually looks like to try to train these linear classifiers via gradient descent. Now as an aside, I'd like\\nto talk about another idea, which is that of image features. So so far we've talked\\nabout linear classifiers, which is just maybe taking\\nour raw image pixels and then feeding the raw pixels themselves into our linear classifier. But as we talked about\\nin the last lecture, this is maybe not such\\na great thing to do, because of things like\\nmulti-modality and whatnot. So in practice, actually\\nfeeding raw pixel values into linear classifiers\\ntends to not work so well. So it was actually common\\nbefore the dominance of deep neural networks, was instead to have\\nthis two-stage approach, where first, you would take your image and then compute various\\nfeature representations of that image, that are maybe computing different kinds of quantities\\nrelating to the appearance of the image, and then concatenate these\\ndifferent feature vectors to give you some feature\\nrepresentation of the image, and now this feature\\nrepresentation of the image would be fed into a linear classifier, rather than feeding the\\nraw pixels themselves into the classifier. And the motivation here is that, so imagine we have a\\ntraining data set on the left of these red points, and\\nred points in the middle and blue points around that. And for this kind of data set, there's no way that we can\\ndraw a linear decision boundary to separate the red points\\nfrom the blue points. And we saw more examples of\\nthis in the last lecture. But if we use a clever feature transform, in this case transforming\\nto polar coordinates, then now after we do\\nthe feature transform, then this complex data\\nset actually might become linearly separable, and actually could be classified correctly by a linear classifier. And the whole trick here\\nnow is to figure out what is the right feature transform that is computing the right quantities for the problem that you care about. So for images, maybe\\nconverting your pixels to polar coordinates, doesn't make sense, but you actually can try to write down feature representations of images that might make sense, and actually might help you out and might do better than\\nputting in raw pixels into the classifier. So one example of this kind\\nof feature representation that's super simple, is this\\nidea of a color histogram. So you'll take maybe each pixel, you'll take this hue color spectrum and divide it into buckets\\nand then for every pixel, you'll map it into one\\nof those color buckets and then count up how many pixels fall into each of these different buckets. So this tells you globally\\nwhat colors are in the image. Maybe if this example of a frog, this feature vector would tell us there's a lot of green stuff, and maybe not a lot of\\npurple or red stuff. And this is kind of a simple feature\\nvector that you might see in practice. Another common feature vector that we saw before the rise of neural networks, or before the dominance of neural networks was this histogram of oriented gradients. So remember from the first lecture, that Hubel and Wiesel\\nfound these oriented edges are really important in\\nthe human visual system, and this histogram of oriented gradients feature representation tries to capture the same intuition and\\nmeasure the local orientation of edges on the image. So what this thing is going to do, is take our image and then divide it into these little eight\\nby eight pixel regions. And then within each of those\\neight by eight pixel regions, we'll compute what is the\\ndominant edge direction of each pixel, quantize\\nthose edge directions into several buckets and then\\nwithin each of those regions, compute a histogram over these\\ndifferent edge orientations. And now your full-feature vector will be these different\\nbucketed histograms of edge orientations across all the different\\neight by eight regions in the image. So this is in some sense dual to the color histogram\\nclassifier that we saw before. So color histogram is\\nsaying, globally, what colors exist in the image, and this is saying, overall,\\nwhat types of edge information exist in the image. And even localized to\\ndifferent parts of the image, what types of edges exist\\nin different regions. So maybe for this frog on the left, you can see he's sitting on a leaf, and these leaves have these\\ndominant diagonal edges, and if you visualize the\\nhistogram of oriented gradient features, then you can\\nsee that in this region, we've got a lot of diagonal edges, that this histogram of oriented gradient feature representation's capturing. So this was a super common\\nfeature representation and was used a lot for object recognition actually not too long ago. Another feature representation\\nthat you might see out there is this idea of bag of words. So this is taking inspiration from natural language processing. So if you've got a paragraph, then a way that you might\\nrepresent a paragraph by a feature vector is\\ncounting up the occurrences of different words in that paragraph. So we want to take that\\nintuition and apply it to images in some way. But the problem is that\\nthere's no really simple, straightforward analogy\\nof words to images, so we need to define our own vocabulary of visual words. So we take this two-stage approach, where first we'll get a bunch of images, sample a whole bunch of tiny random crops from those images and then cluster them using something like K means to come up with these\\ndifferent cluster centers that are maybe representing\\ndifferent types of visual words in the images. So if you look at this\\nexample on the right here, this is a real example of clustering actually different image\\npatches from images, and you can see that after\\nthis clustering step, our visual words capture\\nthese different colors, like red and blue and yellow, as well as these different\\ntypes of oriented edges in different directions, which is interesting that\\nnow we're starting to see these oriented edges\\ncome out from the data in a data-driven way. And now, once we've got\\nthese set of visual words, also called a codebook, then we can encode our\\nimage by trying to say, for each of these visual words, how much does this visual\\nword occur in the image? And now this gives us, again, some slightly different information about what is the visual\\nappearance of this image. And actually this is a type\\nof feature representation that Fei-Fei worked on when\\nshe was a grad student, so this is something\\nthat you saw in practice not too long ago. So then as a bit of teaser, tying this all back together, the way that this image\\nclassification pipeline might have looked like, maybe about five to 10 years ago, would be that you would take your image, and then compute these different\\nfeature representations of your image, things like bag of words, or histogram of orientated gradients, concatenate a whole bunch\\nof features together, and then feed these feature extractors down into some linear classifier. I'm simplifying a little bit, the pipelines were a little\\nbit more complex than that, but this is the general intuition. And then the idea here was\\nthat after you extracted these features, this feature extractor would be a fixed block\\nthat would not be updated during training. And during training, you would only update\\nthe linear classifier if it's working on top of features. And actually, I would\\nargue that once we move to convolutional neural networks, and these deep neural networks, it actually doesn't look that different. The only difference is that\\nrather than writing down the features ahead of time, we're going to learn the\\nfeatures directly from the data. So we'll take our raw pixels and feed them into this to convolutional network, which will end up computing\\nthrough many different layers some type of feature representation driven by the data, and\\nthen we'll actually train this entire weights for\\nthis entire network, rather than just the\\nweights of linear classifier on top. So, next time we'll really\\nstart diving into this idea in more detail, and we'll\\nintroduce some neural networks, and start talking about\\nbackpropagation as well.\", metadata={'source': 'h7iBpEHGVNc', 'title': 'Lecture 3 | Loss Functions and Optimization', 'description': 'Unknown', 'view_count': 858351, 'thumbnail_url': 'https://i.ytimg.com/vi/h7iBpEHGVNc/hq720.jpg', 'publish_date': '2017-08-11 00:00:00', 'length': 4480, 'author': 'Stanford University School of Engineering'})]\n"
     ]
    }
   ],
   "source": [
    "print(transcripts[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "708ead3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(len(transcripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b19d247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing transcript: Lecture 1 - Introduction to Convolutional Neural Networks for Visual Recognition\n",
      "Processing transcript: Lecture 2 - Image Classification\n",
      "Processing transcript: Lecture 3 - Loss Functions and Optimization\n",
      "Processing transcript: Lecture 4 - Introduction to Neural Networks\n",
      "Processing transcript: Lecture 5 - Convolutional Neural Networks\n",
      "Processing transcript: Lecture 6 - Training Neural Networks I\n",
      "Processing transcript: Lecture 7 - Training Neural Networks II\n",
      "Processing transcript: Lecture 8 - Deep Learning Software\n",
      "Processing transcript: Lecture 9 - CNN Architectures\n",
      "Processing transcript: Lecture 10 - Recurrent Neural Networks\n",
      "Processing transcript: Lecture 11 - Detection and Segmentation\n",
      "Processing transcript: Lecture 12 - Visualizing and Understanding\n",
      "Processing transcript: Lecture 13 - Generative Models\n",
      "Processing transcript: Lecture 14 - Deep Reinforcement Learning\n",
      "Processing transcript: Lecture 15 - Efficient Methods and Hardware for Deep Learning\n",
      "Processing transcript: Lecture 16 - Adversarial Examples and Adversarial Training\n"
     ]
    }
   ],
   "source": [
    "for item in transcripts:\n",
    "    lec = item[0].metadata['title'].replace(\"|\",\"-\")\n",
    "    filename = f\"{lec}.txt\"\n",
    "    print(f\"Processing transcript: {lec}\")  \n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        u = item[0].page_content\n",
    "        file.write(u)\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
